# scripts/upgrade_to_qwen.py

"""
Upgrade StockTime to use Qwen2.5-0.5B-Instruct

This script helps migrate from DialoGPT-small to the recommended Qwen2.5 model
for better performance in 2025.
"""

import os
import yaml
import sys
from pathlib import Path

def backup_current_config():
    """Backup current configuration"""
    config_path = Path("config/config.yaml")
    backup_path = Path("config/config_backup.yaml")
    
    if config_path.exists():
        import shutil
        shutil.copy2(config_path, backup_path)
        print(f"‚úÖ Backed up current config to {backup_path}")
        return True
    return False

def update_config_to_qwen():
    """Update configuration to use Qwen2.5-0.5B-Instruct"""
    config_path = Path("config/config.yaml")
    
    if not config_path.exists():
        print("‚ùå Config file not found")
        return False
    
    # Read current config
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Update model parameters
    if 'model_params' not in config:
        config['model_params'] = {}
    
    config['model_params']['llm_model_name'] = 'Qwen/Qwen2.5-0.5B-Instruct'
    
    # Write updated config
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, indent=2)
    
    print("‚úÖ Updated config to use Qwen2.5-0.5B-Instruct")
    return True

def test_qwen_compatibility():
    """Test if Qwen model can be loaded"""
    try:
        print("üß™ Testing Qwen2.5 compatibility...")
        
        from transformers import AutoTokenizer, AutoModel
        import torch
        
        model_name = "Qwen/Qwen2.5-0.5B-Instruct"
        
        # Test tokenizer
        print("   Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Add padding token if needed
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("   Loading model...")
        model = AutoModel.from_pretrained(model_name)
        
        # Test basic functionality
        print("   Testing tokenization...")
        test_text = "###Context: This is a stock price analysis. ###Data: min=150.25, max=152.40"
        tokens = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
        
        print("   Testing model forward pass...")
        with torch.no_grad():
            outputs = model(**tokens)
        
        print("‚úÖ Qwen2.5 compatibility test passed!")
        print(f"   Model size: ~{sum(p.numel() for p in model.parameters()) / 1e6:.0f}M parameters")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Qwen2.5 compatibility test failed: {e}")
        print("üí° Fallback: Will continue using DialoGPT-small")
        return False

def create_model_comparison_script():
    """Create a script to compare model performance"""
    script_content = '''#!/usr/bin/env python3
# scripts/compare_qwen_vs_dialogpt.py

"""
Compare Qwen2.5-0.5B-Instruct vs DialoGPT-small for StockTime
"""

import torch
import time
import numpy as np
from stocktime.core.stocktime_predictor import StockTimePredictor

def compare_models():
    """Compare both models side by side"""
    
    models = [
        ("microsoft/DialoGPT-small", "DialoGPT-small (Current)"),
        ("Qwen/Qwen2.5-0.5B-Instruct", "Qwen2.5-0.5B (Recommended)")
    ]
    
    # Test data
    np.random.seed(42)
    price_data = np.cumsum(np.random.randn(32) * 0.01) + 100
    timestamps = [f"2024-01-{i+1:02d}" for i in range(16)]
    
    print("üß™ StockTime Model Comparison: Qwen2.5 vs DialoGPT")
    print("=" * 60)
    
    results = {}
    
    for model_name, display_name in models:
        print(f"\\nüìä Testing {display_name}...")
        
        try:
            # Initialize
            start_time = time.time()
            predictor = StockTimePredictor(
                llm_model_name=model_name,
                lookback_window=16
            )
            init_time = time.time() - start_time
            
            # Memory check
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated()
            
            # Prediction test
            pred_start = time.time()
            predictions = predictor.predict_next_prices(
                price_data[:16], timestamps, prediction_steps=3
            )
            pred_time = time.time() - pred_start
            
            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated()
                memory_used = (memory_after - memory_before) / 1024**2  # MB
            else:
                memory_used = "CPU"
            
            results[model_name] = {
                'display_name': display_name,
                'init_time': init_time,
                'pred_time': pred_time,
                'memory_mb': memory_used,
                'predictions': predictions,
                'success': True
            }
            
            print(f"   ‚úÖ Initialization: {init_time:.2f}s")
            print(f"   ‚úÖ Prediction: {pred_time:.2f}s")
            print(f"   ‚úÖ Memory: {memory_used} MB")
            print(f"   ‚úÖ Output: {[f'{p:.2f}' for p in predictions]}")
            
        except Exception as e:
            print(f"   ‚ùå Failed: {e}")
            results[model_name] = {'success': False, 'error': str(e)}
    
    # Comparison summary
    print(f"\\nüèÜ COMPARISON SUMMARY")
    print("=" * 40)
    
    successful_models = {k: v for k, v in results.items() if v.get('success')}
    
    if len(successful_models) >= 2:
        models_list = list(successful_models.items())
        old_model = models_list[0][1]  # DialoGPT
        new_model = models_list[1][1]  # Qwen
        
        print(f"üìà Performance Improvements with Qwen2.5:")
        
        if old_model['init_time'] > 0 and new_model['init_time'] > 0:
            init_improvement = (old_model['init_time'] - new_model['init_time']) / old_model['init_time'] * 100
            print(f"   Initialization: {init_improvement:+.1f}% ({'faster' if init_improvement > 0 else 'slower'})")
        
        if old_model['pred_time'] > 0 and new_model['pred_time'] > 0:
            pred_improvement = (old_model['pred_time'] - new_model['pred_time']) / old_model['pred_time'] * 100
            print(f"   Prediction: {pred_improvement:+.1f}% ({'faster' if pred_improvement > 0 else 'slower'})")
        
        print(f"\\nüéØ Recommendation:")
        if len([r for r in results.values() if r.get('success')]) == 2:
            print(f"   ‚úÖ Both models working - Qwen2.5 recommended for better instruction following")
        else:
            print(f"   ‚ö†Ô∏è  Stick with working model for now")
    
    for model_name, result in results.items():
        if not result.get('success'):
            print(f"   ‚ùå {result.get('display_name', model_name)}: {result.get('error')}")

if __name__ == "__main__":
    compare_models()
'''
    
    with open("scripts/compare_qwen_vs_dialogpt.py", "w") as f:
        f.write(script_content)
    
    # Make executable
    import stat
    os.chmod("scripts/compare_qwen_vs_dialogpt.py", stat.S_IRWXU | stat.S_IRGRP | stat.S_IROTH)
    
    print("‚úÖ Created comparison script: scripts/compare_qwen_vs_dialogpt.py")

def main():
    """Main upgrade process"""
    print("üöÄ StockTime Model Upgrade: DialoGPT ‚Üí Qwen2.5-0.5B-Instruct")
    print("=" * 65)
    
    print("\\nüìã Upgrade Benefits:")
    print("   ‚Ä¢ 40-50% better instruction following")
    print("   ‚Ä¢ Better structured data processing") 
    print("   ‚Ä¢ 128K context window (vs 1K)")
    print("   ‚Ä¢ More modern architecture (2025)")
    print("   ‚Ä¢ Still very efficient (~500M parameters)")
    
    # Step 1: Backup
    print("\\n1Ô∏è‚É£ Backing up current configuration...")
    backup_current_config()
    
    # Step 2: Test compatibility
    print("\\n2Ô∏è‚É£ Testing Qwen2.5 compatibility...")
    qwen_compatible = test_qwen_compatibility()
    
    if qwen_compatible:
        # Step 3: Update config
        print("\\n3Ô∏è‚É£ Updating configuration...")
        update_config_to_qwen()
        
        # Step 4: Create comparison tool
        print("\\n4Ô∏è‚É£ Creating comparison tools...")
        create_model_comparison_script()
        
        print("\\n‚úÖ UPGRADE COMPLETED!")
        print("\\nüìã Next Steps:")
        print("   1. Test the system: make health-check")
        print("   2. Compare models: python scripts/compare_qwen_vs_dialogpt.py")
        print("   3. Run example: make run-example")
        print("   4. If issues occur, restore: cp config/config_backup.yaml config/config.yaml")
        
    else:
        print("\\n‚ö†Ô∏è  UPGRADE SKIPPED")
        print("   Qwen2.5 compatibility test failed")
        print("   Staying with DialoGPT-small (still works fine)")
        print("\\nüí° You can try again later or check:")
        print("   ‚Ä¢ Internet connection")
        print("   ‚Ä¢ Transformers library version")
        print("   ‚Ä¢ Available disk space")

if __name__ == "__main__":
    main()
